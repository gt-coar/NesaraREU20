{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1F4t6tm1CdFW"
   },
   "source": [
    "# TRPO : Continuous states and Discrete Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cmpnCM1ACE9w",
    "outputId": "e42adb1f-74e6-4605-a09f-c1e9d33b0a03"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torch.nn.utils.convert_parameters import vector_to_parameters, parameters_to_vector\n",
    "import scipy.signal as signal\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import multiprocessing\n",
    "from replay_memory import Memory\n",
    "from torch1 import *\n",
    "import math\n",
    "import time\n",
    "import scipy.optimize\n",
    "from zfilter import RunningStat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njcgl4Uyodo_"
   },
   "outputs": [],
   "source": [
    "def to_device(device, *args):\n",
    "    return [x.to(device) for x in args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pwrYiGndvno"
   },
   "outputs": [],
   "source": [
    "class ZFilter:\n",
    "    \"\"\"\n",
    "    y = (x-mean)/std\n",
    "    using running estimates of mean,std\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n",
    "        self.demean = demean\n",
    "        self.destd = destd\n",
    "        self.clip = clip\n",
    "\n",
    "        self.rs = RunningStat(shape)\n",
    "        self.fix = False\n",
    "\n",
    "    def __call__(self, x, update=True):\n",
    "        if update and not self.fix:\n",
    "            self.rs.push(x)\n",
    "        if self.demean:\n",
    "            x = x - self.rs.mean\n",
    "        if self.destd:\n",
    "            x = x / (self.rs.std + 1e-8)\n",
    "        if self.clip:\n",
    "            x = np.clip(x, -self.clip, self.clip)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8iUJDa65dv_c"
   },
   "outputs": [],
   "source": [
    "class DiscretePolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_num, hidden_size=(64,64), activation='tanh'):\n",
    "        super().__init__()\n",
    "        self.is_disc_action = True\n",
    "        if activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'relu':\n",
    "            self.activation = torch.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "\n",
    "        self.affine_layers = nn.ModuleList()\n",
    "        last_dim = state_dim\n",
    "        for nh in hidden_size:\n",
    "            self.affine_layers.append(nn.Linear(last_dim, nh))\n",
    "            last_dim = nh\n",
    "\n",
    "        self.action_head = nn.Linear(last_dim, action_num)\n",
    "        self.action_head.weight.data.mul_(0.1)\n",
    "        self.action_head.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        for affine in self.affine_layers:\n",
    "            x = self.activation(affine(x))\n",
    "        #print(self.action_head(x))\n",
    "        action_prob = torch.softmax(self.action_head(x), dim=1)\n",
    "        return action_prob\n",
    "\n",
    "    def select_action(self, x):\n",
    "        action_prob = self.forward(x)\n",
    "        action = action_prob.multinomial(1)\n",
    "        return action\n",
    "\n",
    "    def get_kl(self, x):\n",
    "        action_prob1 = self.forward(x)\n",
    "        action_prob0 = action_prob1.detach()\n",
    "        kl = action_prob0 * (torch.log(action_prob0) - torch.log(action_prob1))\n",
    "        return kl.sum(1, keepdim=True)\n",
    "\n",
    "    def get_log_prob(self, x, actions):\n",
    "        action_prob = self.forward(x)\n",
    "        return torch.log(action_prob.gather(1, actions.long().unsqueeze(1)))\n",
    "\n",
    "    def get_fim(self, x):\n",
    "        action_prob = self.forward(x)\n",
    "        M = action_prob.pow(-1).view(-1).detach()\n",
    "        return M, action_prob, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZdJepaDdwHJ"
   },
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_size=(64,64), activation='tanh'):\n",
    "        super().__init__()\n",
    "        if activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'relu':\n",
    "            self.activation = torch.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "\n",
    "        self.affine_layers = nn.ModuleList()\n",
    "        last_dim = state_dim\n",
    "        for nh in hidden_size:\n",
    "            self.affine_layers.append(nn.Linear(last_dim, nh))\n",
    "            last_dim = nh\n",
    "\n",
    "        self.value_head = nn.Linear(last_dim, 1)\n",
    "        self.value_head.weight.data.mul_(0.1)\n",
    "        self.value_head.bias.data.mul_(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        for affine in self.affine_layers:\n",
    "            x = self.activation(affine(x))\n",
    "\n",
    "        value = self.value_head(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKDpl4s2rR-I"
   },
   "outputs": [],
   "source": [
    "def collect_samples(pid, queue, env, policy, custom_reward,\n",
    "                    mean_action, render, running_state, min_batch_size):\n",
    "    torch.randn(pid)\n",
    "    log = dict()\n",
    "    memory = Memory()\n",
    "    num_steps = 0\n",
    "    total_reward = 0\n",
    "    min_reward = 1e6\n",
    "    max_reward = -1e6\n",
    "    total_c_reward = 0\n",
    "    min_c_reward = 1e6\n",
    "    max_c_reward = -1e6\n",
    "    num_episodes = 0\n",
    "\n",
    "    while num_steps < min_batch_size:\n",
    "        state = env.reset()\n",
    "        if running_state is not None:\n",
    "            state = running_state(state)\n",
    "        reward_episode = 0\n",
    "\n",
    "        for t in range(10000):\n",
    "            state_var = tensor(state).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                if mean_action:\n",
    "                    action = policy(state_var)[0][0].numpy()\n",
    "                else:\n",
    "                    action = policy.select_action(state_var)[0].numpy()\n",
    "            action = int(action) if policy.is_disc_action else action.astype(np.float64)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward_episode += reward\n",
    "            if running_state is not None:\n",
    "                next_state = running_state(next_state)\n",
    "\n",
    "            if custom_reward is not None:\n",
    "                reward = custom_reward(state, action)\n",
    "                total_c_reward += reward\n",
    "                min_c_reward = min(min_c_reward, reward)\n",
    "                max_c_reward = max(max_c_reward, reward)\n",
    "\n",
    "            mask = 0 if done else 1\n",
    "\n",
    "            memory.push(state, action, mask, next_state, reward)\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # log stats\n",
    "        num_steps += (t + 1)\n",
    "        num_episodes += 1\n",
    "        total_reward += reward_episode\n",
    "        min_reward = min(min_reward, reward_episode)\n",
    "        max_reward = max(max_reward, reward_episode)\n",
    "\n",
    "    log['num_steps'] = num_steps\n",
    "    log['num_episodes'] = num_episodes\n",
    "    log['total_reward'] = total_reward\n",
    "    log['avg_reward'] = total_reward / num_episodes\n",
    "    log['max_reward'] = max_reward\n",
    "    log['min_reward'] = min_reward\n",
    "    if custom_reward is not None:\n",
    "        log['total_c_reward'] = total_c_reward\n",
    "        log['avg_c_reward'] = total_c_reward / num_steps\n",
    "        log['max_c_reward'] = max_c_reward\n",
    "        log['min_c_reward'] = min_c_reward\n",
    "\n",
    "    if queue is not None:\n",
    "        queue.put([pid, memory, log])\n",
    "    else:\n",
    "        return memory, log\n",
    "\n",
    "\n",
    "def merge_log(log_list):\n",
    "    log = dict()\n",
    "    log['total_reward'] = sum([x['total_reward'] for x in log_list])\n",
    "    log['num_episodes'] = sum([x['num_episodes'] for x in log_list])\n",
    "    log['num_steps'] = sum([x['num_steps'] for x in log_list])\n",
    "    log['avg_reward'] = log['total_reward'] / log['num_episodes']\n",
    "    log['max_reward'] = max([x['max_reward'] for x in log_list])\n",
    "    log['min_reward'] = min([x['min_reward'] for x in log_list])\n",
    "    if 'total_c_reward' in log_list[0]:\n",
    "        log['total_c_reward'] = sum([x['total_c_reward'] for x in log_list])\n",
    "        log['avg_c_reward'] = log['total_c_reward'] / log['num_steps']\n",
    "        log['max_c_reward'] = max([x['max_c_reward'] for x in log_list])\n",
    "        log['min_c_reward'] = min([x['min_c_reward'] for x in log_list])\n",
    "\n",
    "    return log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Xk5aI9HnwHE"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, env, policy, device, custom_reward=None,\n",
    "                 mean_action=False, render=False, running_state=None, num_threads=1):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.device = device\n",
    "        self.custom_reward = custom_reward\n",
    "        self.mean_action = mean_action\n",
    "        self.running_state = running_state\n",
    "        self.render = render\n",
    "        self.num_threads = num_threads\n",
    "\n",
    "    def collect_samples(self, min_batch_size):\n",
    "        t_start = time.time()\n",
    "        to_device(torch.device('cpu'), self.policy)\n",
    "        thread_batch_size = int(math.floor(min_batch_size / self.num_threads))\n",
    "        queue = multiprocessing.Queue()\n",
    "        workers = []\n",
    "\n",
    "        for i in range(self.num_threads-1):\n",
    "            worker_args = (i+1, queue, self.env, self.policy, self.custom_reward, self.mean_action,\n",
    "                           False, self.running_state, thread_batch_size)\n",
    "            workers.append(multiprocessing.Process(target=collect_samples, args=worker_args))\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "\n",
    "        memory, log = collect_samples(0, None, self.env, self.policy, self.custom_reward, self.mean_action,\n",
    "                                      self.render, self.running_state, thread_batch_size)\n",
    "\n",
    "        worker_logs = [None] * len(workers)\n",
    "        worker_memories = [None] * len(workers)\n",
    "        for _ in workers:\n",
    "            pid, worker_memory, worker_log = queue.get()\n",
    "            worker_memories[pid - 1] = worker_memory\n",
    "            worker_logs[pid - 1] = worker_log\n",
    "        for worker_memory in worker_memories:\n",
    "            memory.append(worker_memory)\n",
    "            \n",
    "        batch = memory.sample()\n",
    "        if self.num_threads > 1:\n",
    "            log_list = [log] + worker_logs\n",
    "            log = merge_log(log_list)\n",
    "        to_device(self.device, self.policy)\n",
    "        t_end = time.time()\n",
    "        log['sample_time'] = t_end - t_start\n",
    "        log['action_mean'] = np.mean(np.vstack(batch.action), axis=0)\n",
    "        log['action_min'] = np.min(np.vstack(batch.action), axis=0)\n",
    "        log['action_max'] = np.max(np.vstack(batch.action), axis=0)\n",
    "        return batch, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe4og2fEnwJ6"
   },
   "outputs": [],
   "source": [
    "def estimate_advantages(rewards, masks, values, gamma, tau, device):\n",
    "    rewards, masks, values = to_device(torch.device('cpu'), rewards, masks, values)\n",
    "    tensor_type = type(rewards)\n",
    "    deltas = tensor_type(rewards.size(0), 1)\n",
    "    advantages = tensor_type(rewards.size(0), 1)\n",
    "\n",
    "    prev_value = 0\n",
    "    prev_advantage = 0\n",
    "    for i in reversed(range(rewards.size(0))):\n",
    "        deltas[i] = rewards[i] + gamma * prev_value * masks[i] - values[i]\n",
    "        advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n",
    "\n",
    "        prev_value = values[i, 0]\n",
    "        prev_advantage = advantages[i, 0]\n",
    "\n",
    "    returns = values + advantages\n",
    "    advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "    advantages, returns = to_device(device, advantages, returns)\n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ub19XasunwPN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def conjugate_gradients(Avp_f, b, nsteps, rdotr_tol=1e-10):\n",
    "    x = zeros(b.size(), device=b.device)\n",
    "    r = b.clone()\n",
    "    p = b.clone()\n",
    "    rdotr = torch.dot(r, r)\n",
    "    for i in range(nsteps):\n",
    "        Avp = Avp_f(p)\n",
    "        alpha = rdotr / torch.dot(p, Avp)\n",
    "        x += alpha * p\n",
    "        r -= alpha * Avp\n",
    "        new_rdotr = torch.dot(r, r)\n",
    "        betta = new_rdotr / rdotr\n",
    "        p = r + betta * p\n",
    "        rdotr = new_rdotr\n",
    "        if rdotr < rdotr_tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "\n",
    "def line_search(model, f, x, fullstep, expected_improve_full, max_backtracks=10, accept_ratio=0.1):\n",
    "    fval = f(True).item()\n",
    "\n",
    "    for stepfrac in [.5**x for x in range(max_backtracks)]:\n",
    "        x_new = x + stepfrac * fullstep\n",
    "        set_flat_params_to(model, x_new)\n",
    "        fval_new = f(True).item()\n",
    "        actual_improve = fval - fval_new\n",
    "        expected_improve = expected_improve_full * stepfrac\n",
    "        ratio = actual_improve / expected_improve\n",
    "\n",
    "        if ratio > accept_ratio:\n",
    "            return True, x_new\n",
    "    return False, x\n",
    "\n",
    "\n",
    "def trpo_step(policy_net, value_net, states, actions, returns, advantages, max_kl, damping, l2_reg, use_fim=True):\n",
    "\n",
    "    \"\"\"update critic\"\"\"\n",
    "\n",
    "    def get_value_loss(flat_params):\n",
    "        set_flat_params_to(value_net, tensor(flat_params))\n",
    "        for param in value_net.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.fill_(0)\n",
    "        values_pred = value_net(states)\n",
    "        value_loss = (values_pred - returns).pow(2).mean()\n",
    "\n",
    "        # weight decay\n",
    "        for param in value_net.parameters():\n",
    "            value_loss += param.pow(2).sum() * l2_reg\n",
    "        value_loss.backward()\n",
    "        return value_loss.item(), get_flat_grad_from(value_net.parameters()).cpu().numpy()\n",
    "\n",
    "    flat_params, _, opt_info = scipy.optimize.fmin_l_bfgs_b(get_value_loss,\n",
    "                                                            get_flat_params_from(value_net).detach().cpu().numpy(),\n",
    "                                                            maxiter=25)\n",
    "    set_flat_params_to(value_net, tensor(flat_params))\n",
    "\n",
    "    \"\"\"update policy\"\"\"\n",
    "    with torch.no_grad():\n",
    "        fixed_log_probs = policy_net.get_log_prob(states, actions)\n",
    "    \"\"\"define the loss function for TRPO\"\"\"\n",
    "    def get_loss(volatile=False):\n",
    "        with torch.set_grad_enabled(not volatile):\n",
    "            log_probs = policy_net.get_log_prob(states, actions)\n",
    "            action_loss = -advantages * torch.exp(log_probs - fixed_log_probs)\n",
    "            return action_loss.mean()\n",
    "\n",
    "    \"\"\"use fisher information matrix for Hessian*vector\"\"\"\n",
    "    def Fvp_fim(v):\n",
    "        M, mu, info = policy_net.get_fim(states)\n",
    "        mu = mu.view(-1)\n",
    "        filter_input_ids = set() if policy_net.is_disc_action else set([info['std_id']])\n",
    "\n",
    "        t = ones(mu.size(), requires_grad=True, device=mu.device)\n",
    "        mu_t = (mu * t).sum()\n",
    "        Jt = compute_flat_grad(mu_t, policy_net.parameters(), filter_input_ids=filter_input_ids, create_graph=True)\n",
    "        Jtv = (Jt * v).sum()\n",
    "        Jv = torch.autograd.grad(Jtv, t)[0]\n",
    "        MJv = M * Jv.detach()\n",
    "        mu_MJv = (MJv * mu).sum()\n",
    "        JTMJv = compute_flat_grad(mu_MJv, policy_net.parameters(), filter_input_ids=filter_input_ids).detach()\n",
    "        JTMJv /= states.shape[0]\n",
    "        if not policy_net.is_disc_action:\n",
    "            std_index = info['std_index']\n",
    "            JTMJv[std_index: std_index + M.shape[0]] += 2 * v[std_index: std_index + M.shape[0]]\n",
    "        return JTMJv + v * damping\n",
    "\n",
    "    \"\"\"directly compute Hessian*vector from KL\"\"\"\n",
    "    def Fvp_direct(v):\n",
    "        kl = policy_net.get_kl(states)\n",
    "        kl = kl.mean()\n",
    "\n",
    "        grads = torch.autograd.grad(kl, policy_net.parameters(), create_graph=True)\n",
    "        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
    "\n",
    "        kl_v = (flat_grad_kl * v).sum()\n",
    "        grads = torch.autograd.grad(kl_v, policy_net.parameters())\n",
    "        flat_grad_grad_kl = torch.cat([grad.contiguous().view(-1) for grad in grads]).detach()\n",
    "\n",
    "        return flat_grad_grad_kl + v * damping\n",
    "\n",
    "    Fvp = Fvp_fim if use_fim else Fvp_direct\n",
    "\n",
    "    loss = get_loss()\n",
    "    grads = torch.autograd.grad(loss, policy_net.parameters())\n",
    "    loss_grad = torch.cat([grad.view(-1) for grad in grads]).detach()\n",
    "    stepdir = conjugate_gradients(Fvp, -loss_grad, 10)\n",
    "\n",
    "    shs = 0.5 * (stepdir.dot(Fvp(stepdir)))\n",
    "    lm = math.sqrt(max_kl / shs)\n",
    "    fullstep = stepdir * lm\n",
    "    expected_improve = -loss_grad.dot(fullstep)\n",
    "\n",
    "    prev_params = get_flat_params_from(policy_net)\n",
    "    success, new_params = line_search(policy_net, get_loss, prev_params, fullstep, expected_improve)\n",
    "    set_flat_params_to(policy_net, new_params)\n",
    "\n",
    "    return success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hm_Dw9q4nwUM"
   },
   "outputs": [],
   "source": [
    "env_name = \"MountainCar-v0\"\n",
    "seed = 0\n",
    "model_path = None\n",
    "log_std = -0.0\n",
    "render = False\n",
    "num_threads = 4\n",
    "gamma = 0.99\n",
    "tau = 0.95\n",
    "max_kl = 1e-2\n",
    "damping = 1e-2\n",
    "l2_reg = 1e-3\n",
    "max_iter_num = 100\n",
    "min_batch_size = 2048\n",
    "log_interval = 10\n",
    "save_model_interval = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "Zkwa3qy5DBqS",
    "outputId": "333548a1-55b4-4cfb-8141-7668af99d746"
   },
   "outputs": [],
   "source": [
    "dtype = torch.float64\n",
    "gpu_index=0\n",
    "torch.set_default_dtype(dtype)\n",
    "device = torch.device('cuda', index=gpu_index) if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(gpu_index)\n",
    "\n",
    "\"\"\"environment\"\"\"\n",
    "env = gym.make(env_name)\n",
    "#env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "is_disc_action = len(env.action_space.shape) == 0\n",
    "running_state = ZFilter((state_dim,), clip=5)\n",
    "# running_reward = ZFilter((1,), demean=False, clip=10)\n",
    "\n",
    "\"\"\"seeding\"\"\"\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "\"\"\"define actor and critic\"\"\"\n",
    "if model_path is None:\n",
    "    if is_disc_action:\n",
    "        policy_net = DiscretePolicy(state_dim, env.action_space.n)\n",
    "    else:\n",
    "        policy_net = Policy(state_dim, env.action_space.shape[0], log_std=log_std)\n",
    "    value_net = Value(state_dim)\n",
    "else:\n",
    "    policy_net, value_net, running_state = pickle.load(open(model_path, \"rb\"))\n",
    "policy_net.to(device)\n",
    "value_net.to(device)\n",
    "\n",
    "\"\"\"create agent\"\"\"\n",
    "agent = Agent(env, policy_net, device, running_state=running_state, render=render, num_threads=num_threads)\n",
    "\n",
    "\n",
    "def update_params(batch):\n",
    "    states = torch.from_numpy(np.stack(batch.state)).to(dtype).to(device)\n",
    "    actions = torch.from_numpy(np.stack(batch.action)).to(dtype).to(device)\n",
    "    rewards = torch.from_numpy(np.stack(batch.reward)).to(dtype).to(device)\n",
    "    masks = torch.from_numpy(np.stack(batch.mask)).to(dtype).to(device)\n",
    "    with torch.no_grad():\n",
    "        values = value_net(states)\n",
    "\n",
    "    \"\"\"get advantage estimation from the trajectories\"\"\"\n",
    "    advantages, returns = estimate_advantages(rewards, masks, values, gamma, tau, device)\n",
    "\n",
    "    \"\"\"perform TRPO update\"\"\"\n",
    "    trpo_step(policy_net, value_net, states, actions, returns, advantages, max_kl, damping, l2_reg)\n",
    "\n",
    "\n",
    "def main_loop():\n",
    "    logs = []\n",
    "    for i_iter in range(max_iter_num):\n",
    "        \"\"\"generate multiple trajectories that reach the minimum batch_size\"\"\"\n",
    "        batch, log = agent.collect_samples(min_batch_size)\n",
    "        logs.append(log)\n",
    "        t0 = time.time()\n",
    "        update_params(batch)\n",
    "        t1 = time.time()\n",
    "\n",
    "        if i_iter % log_interval == 0:\n",
    "            print('{}\\tT_sample {:.4f}\\tT_update {:.4f}\\tR_min {:.2f}\\tR_max {:.2f}\\tR_avg {:.2f}'.format(\n",
    "                i_iter, log['sample_time'], t1-t0, log['min_reward'], log['max_reward'], log['avg_reward']))\n",
    "\n",
    "        if save_model_interval > 0 and (i_iter+1) % save_model_interval == 0:\n",
    "            to_device(torch.device('cpu'), policy_net, value_net)\n",
    "            pickle.dump((policy_net, value_net, running_state),\n",
    "                        open(os.path.join(assets_dir(), 'learned_models/{}_trpo.p'.format(env_name)), 'wb'))\n",
    "            to_device(device, policy_net, value_net)\n",
    "\n",
    "        \"\"\"clean up gpu memory\"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "    return logs\n",
    "\n",
    "\n",
    "logs = main_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "colab_type": "code",
    "id": "-uAVGhqPujw0",
    "outputId": "9bb06c4d-d903-4ff0-ac68-67271fe0fa8e"
   },
   "outputs": [],
   "source": [
    "avg_reward =[]\n",
    "max_reward =[]\n",
    "min_reward =[]\n",
    "for log in logs:\n",
    "  avg_reward.append(log['avg_reward'])\n",
    "  max_reward.append(log['max_reward'])\n",
    "  min_reward.append(log['min_reward'])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4APOV9WBb-i9"
   },
   "outputs": [],
   "source": [
    "def plot(p1,p2,p3,iterations):\n",
    "    \n",
    "    x=np.array(range(1,iterations+1))\n",
    "    #y=total_rewards\n",
    "    plt.tick_params(axis='y',labelsize=9)\n",
    "    plt.tick_params(axis='x',labelsize=9)\n",
    "    plt.xlabel('Iterations', fontsize=12)\n",
    "    plt.plot(x,p1, label='Average Total Reward/Iteration')\n",
    "    plt.plot(x,p2,label='Max Total Reward/Iteration') \n",
    "    plt.plot(x,p3,label='Min Total Reward/Iteration') \n",
    "\n",
    "    plt.title('MountainCar-v0 TRPO',fontsize=12)\n",
    "    fig_size=(10,7)\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    plt.legend(loc='lower right') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "colab_type": "code",
    "id": "BVzgf_EgvjWl",
    "outputId": "5c731254-9ad8-46bb-8140-a088463122de"
   },
   "outputs": [],
   "source": [
    "plot(avg_reward,max_reward,min_reward,len(avg_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzwCbQsAzVHH"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env = wrappers.Monitor(env, \"./gym-results-mountaincar\", force=True)\n",
    "for _ in range(10):\n",
    "    reward_episode = 0\n",
    "    state = env.reset()\n",
    "    if running_state is not None:\n",
    "        state = running_state(state)\n",
    "    for _ in range(201):\n",
    "        state_var = tensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = policy_net.select_action(state_var)[0].numpy()\n",
    "            action = int(action) \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = running_state(next_state)\n",
    "            state = next_state\n",
    "            reward_episode += reward\n",
    "        if done: \n",
    "            print(reward_episode)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TRPO_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

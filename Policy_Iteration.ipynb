{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *pymdptoolbox* is a python library from which MDP examples can be imported </br>\n",
    "##### Documentation of *pymdptoolbox* : https://pymdptoolbox.readthedocs.io/en/latest/api/mdptoolbox.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymdptoolbox in /opt/anaconda3/envs/ComputerVision/lib/python3.7/site-packages (4.0b3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/ComputerVision/lib/python3.7/site-packages (from pymdptoolbox) (1.18.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/ComputerVision/lib/python3.7/site-packages (from pymdptoolbox) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymdptoolbox   #Install pymdptoolbox within the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox.example\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Functions to be implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Define environment </br>\n",
    "-Policy Evaluation </br>\n",
    "-Bellman Update </br>\n",
    "-Choose Greedy action and update policy </br>\n",
    "-Policy Improvement </br>\n",
    "-Policy Iteration </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class MarkovDP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class MarkovDP contains the following attributes:\n",
    "1)Number of states  : s\n",
    "2)Number of actions : a\n",
    "3)State Space\n",
    "4)Action Space\n",
    "5)Transition probability matrix of size (a,s,s)\n",
    "6)Reward matrix (a,s,s)\n",
    "'''\n",
    "class MarkovDP:\n",
    "    def __init__(self,s,a):\n",
    "        self.num_state             = s\n",
    "        self.num_action            = a\n",
    "        self.states                = np.array(range(0,s))\n",
    "        self.actions               = np.array(range(0,a))\n",
    "        self.transitions           = np.zeros((a,s,s))\n",
    "        self.rewards               = np.zeros((a,s,s))\n",
    "        \n",
    "# The function below initializes transition probability matrix and rewards marks \n",
    "\n",
    "    def initialize_mdp(self):      \n",
    "        np.random.seed(0)        #for reproducibility \n",
    "        self.transitions, self.rewards = mdptoolbox.example.rand(self.num_state,self.num_action)\n",
    "        self.rewards = np.random.rand(self.num_action,self.num_state,self.num_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, V, pi, gamma, theta):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in env.states:\n",
    "            v = V[s].copy()\n",
    "            V=update_v_policy(env, V, pi, s, gamma)    #Bellman update \n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Update function\n",
    "$$\\large v(s) \\leftarrow \\sum_a \\pi(a | s) \\sum_{s', r} p(s', r | s, a)[r + \\gamma v(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_v(env, V, pi, s, gamma):\n",
    "    sum=0\n",
    "    for a in env.actions:\n",
    "        transitions = np.reshape(env.transitions[a][s][:],(-1,1))\n",
    "        rewards = np.reshape(env.rewards[a][s][:],(-1,1))\n",
    "        sum=sum+pi[s][a]*(np.sum(np.multiply(transitions,rewards)+ gamma * np.multiply(transitions,V)))\n",
    "    V[s]=sum\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that chooses the greedy action for a particular state 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_action(env, V, pi, s, gamma):\n",
    "    q=np.empty((env.num_action,1),dtype=float)\n",
    "    for a in env.actions:\n",
    "        pi[s][a]=0\n",
    "        transitions = np.reshape(env.transitions[a][s][:],(-1,1))\n",
    "        rewards = np.reshape(env.rewards[a][s][:],(-1,1))\n",
    "        q[a]=np.sum(np.multiply(transitions,rewards)+ gamma * np.multiply(transitions,V))\n",
    "    action=np.argmax(q)        #Choose greedy action\n",
    "    pi[s][action]=1            #Update Policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(env, V, pi, gamma):\n",
    "    policy_stable = True        # If policy_stable == True : Policy need not be updated anymore\n",
    "    for s in env.states:\n",
    "        old = pi[s].copy()\n",
    "        choose_best_action(env, V, pi, s, gamma)\n",
    "        if not np.array_equal(pi[s], old): \n",
    "            policy_stable = False\n",
    "    return pi, policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration Function\n",
    "\n",
    "#### Initialize Value function vector : [0,0,0...0]\n",
    "#### Initialize policy : Uniform Probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma, theta):\n",
    "    V = np.zeros((env.num_state,1))          #Initialize Value function vector : [0,0,0...0]\n",
    "    pi = np.ones((env.num_state,env.num_action)) / env.num_action   #Policy Initialization\n",
    "    policy_stable = False\n",
    "    while not policy_stable:\n",
    "        V = evaluate_policy(env, V, pi, gamma, theta)          #Policy Evaluation step\n",
    "        pi, policy_stable = improve_policy(env, V, pi, gamma)  #Policy Iteration step\n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define an MDP Environment : Insantiate Class\n",
    "    Number of states : 10\n",
    "    Number of actions : 3\n",
    "'''\n",
    "env= MarkovDP(10,3)      #Define an MDP Environment : Insantiate Class\n",
    "env.initialize_mdp()    #Define P and R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.12379813194274902 seconds ---\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9       #Discount rate\n",
    "theta = 0.0001    #A small positive number\n",
    "\n",
    "start_time = time.time()\n",
    "V,pi=policy_iteration(env, gamma, theta)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(s 1 ) : 7.573231634060111\n",
      "V(s 2 ) : 7.430471492387628\n",
      "V(s 3 ) : 7.6552394629170335\n",
      "V(s 4 ) : 7.362491475340923\n",
      "V(s 5 ) : 7.678968623420015\n",
      "V(s 6 ) : 7.574400188551869\n",
      "V(s 7 ) : 7.4880983523263\n",
      "V(s 8 ) : 7.557449501942633\n",
      "V(s 9 ) : 7.94906786700345\n",
      "V(s 10 ) : 7.909065762013673\n"
     ]
    }
   ],
   "source": [
    "#Final values of the value function at each state\n",
    "for s in env.states:\n",
    "    print('V(s',s+1,') :',V[s][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(a|s 1 )= [1. 0. 0.]\n",
      "P(a|s 2 )= [1. 0. 0.]\n",
      "P(a|s 3 )= [1. 0. 0.]\n",
      "P(a|s 4 )= [0. 0. 1.]\n",
      "P(a|s 5 )= [0. 0. 1.]\n",
      "P(a|s 6 )= [1. 0. 0.]\n",
      "P(a|s 7 )= [1. 0. 0.]\n",
      "P(a|s 8 )= [0. 0. 1.]\n",
      "P(a|s 9 )= [0. 0. 1.]\n",
      "P(a|s 10 )= [0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Optimal Policy\n",
    "for s in env.states:\n",
    "    print('P(a|s',s+1,')=',pi[s])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
